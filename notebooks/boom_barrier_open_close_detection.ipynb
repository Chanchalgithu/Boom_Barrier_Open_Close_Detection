{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c466ae3b-5bdd-4c1a-b652-daff4b8519e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV: 4.11.0\n",
      "TensorFlow: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"OpenCV:\", cv2.__version__)\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e832a72-99ec-454a-8a15-748d74f5ff38",
   "metadata": {},
   "source": [
    "# Boom Barrier Open–Close Detection using Computer Vision\n",
    "\n",
    "This notebook demonstrates an end-to-end computer vision pipeline to classify boom barrier states as **OPEN** or **CLOSED**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1250f17-8041-401b-b416-7d48978b180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f95ece-c160-4ec6-bdd6-c74024414d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open images: 72\n",
      "Close images: 72\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Open images:\", len(os.listdir(r\"C:\\Users\\hp pc\\Desktop\\dataset\\open\")))\n",
    "print(\"Close images:\", len(os.listdir(r\"C:\\Users\\hp pc\\Desktop\\dataset\\close\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be0f79-3a13-47b3-bcd2-ea68cfd20152",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c53083a1-a7ed-4526-ae12-e649dcf30528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (144, 128, 128, 3)\n",
      "Labels shape: (144, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\hp pc\\Desktop\\dataset\"\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "img_size = 128\n",
    "classes = [\"open\", \"close\"]\n",
    "\n",
    "for label, category in enumerate(classes):\n",
    "    folder_path = os.path.join(dataset_path, category)\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "data = np.array(data) / 255.0\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37808626-bf19-4d7f-b53b-2fa081bf4d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 115\n",
      "Testing samples: 29\n"
     ]
    }
   ],
   "source": [
    "#Train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ce3a41d-0f92-4a41-ba45-4d618c8bfcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 57600)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               7372928   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,392,578\n",
      "Trainable params: 7,392,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build CNN Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaf7c7af-e389-4dc5-ba20-04fc8528126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 6s 1s/step - loss: 2.9961 - accuracy: 0.5304 - val_loss: 0.7214 - val_accuracy: 0.5517\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.6811 - accuracy: 0.6522 - val_loss: 0.7014 - val_accuracy: 0.5517\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.5157 - accuracy: 0.6870 - val_loss: 0.6052 - val_accuracy: 0.7241\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.3875 - accuracy: 0.8174 - val_loss: 0.6558 - val_accuracy: 0.6897\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.3049 - accuracy: 0.8609 - val_loss: 0.5208 - val_accuracy: 0.7931\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c64e664-658e-4029-8e1d-5267f5d27e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 165ms/step - loss: 0.5208 - accuracy: 0.7931\n",
      "Test Accuracy: 0.7931034564971924\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf7279d-b2a8-4285-854e-8c5caec3cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "\n",
    "model.save(\"boom_barrier_open_close_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23c47f18-06a9-43a4-9afd-67135b146039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Boom Barrier Status: OPEN\n"
     ]
    }
   ],
   "source": [
    "# Single Image Prediction (Demo\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "img = cv2.imread(os.path.join(dataset_path, \"open\", os.listdir(os.path.join(dataset_path, \"open\"))[0]))\n",
    "img = cv2.resize(img, (128,128))\n",
    "img = np.expand_dims(img, axis=0) / 255.0\n",
    "\n",
    "prediction = model.predict(img)\n",
    "result = \"OPEN\" if np.argmax(prediction)==0 else \"CLOSED\"\n",
    "\n",
    "print(\"Boom Barrier Status:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596420b0-8642-4a7d-b852-86c5c0fac82b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "A CNN-based model was built to classify boom barrier states as OPEN or CLOSED.\n",
    "The pipeline is scalable with more data and can be integrated with CCTV systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaf5a4-7c34-4872-b5f9-c05ed3d0066d",
   "metadata": {},
   "source": [
    " - The prediction depends on the input image.\n",
    "\n",
    " - When I tested with an open-barrier image, the model correctly predicted OPEN.\n",
    "\n",
    " - Testing with a closed-barrier image results in CLOSED prediction, within expected accuracy limits.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c597834-7964-4599-8fca-aa187d9d03c1",
   "metadata": {},
   "source": [
    "“Although the image belongs to the open class, the model predicted CLOSED with high confidence due to visual features closer to the closed state.\n",
    "This highlights the probabilistic nature of CNNs and real-world ambiguity.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86dc752-3c5f-47ad-b88b-c26b86ea2bd3",
   "metadata": {},
   "source": [
    "## ML Backend Workflow (Boom Barrier Open–Close Detection)\n",
    "\n",
    "1. **Problem Definition**  \n",
    "   Defined a binary image classification problem to detect whether a boom barrier is OPEN or CLOSED.\n",
    "\n",
    "2. **Dataset Collection**  \n",
    "   Collected a balanced open-source image dataset with two classes:\n",
    "   - Open barrier images\n",
    "   - Closed barrier images\n",
    "\n",
    "3. **Dataset Organization**  \n",
    "   Organized images using folder-based labeling:\n",
    "   - dataset/open/\n",
    "   - dataset/close/\n",
    "\n",
    "4. **Light EDA (Exploratory Data Analysis)**  \n",
    "   - Verified image count per class  \n",
    "   - Visualized sample images  \n",
    "   - Ensured dataset balance\n",
    "\n",
    "5. **Data Cleaning (Image-specific)**  \n",
    "   - Manually removed irrelevant or noisy images  \n",
    "   - Ensured correct labeling  \n",
    "   - Kept minor blur/variation to reflect real-world data\n",
    "\n",
    "6. **Data Preprocessing**  \n",
    "   - Resized all images to 128×128  \n",
    "   - Normalized pixel values (0–1 range)  \n",
    "   - Converted labels to categorical format\n",
    "\n",
    "7. **Train–Test Split**  \n",
    "   Split the dataset into training and testing sets (80% train, 20% test).\n",
    "\n",
    "8. **Model Building**  \n",
    "   Built a CNN model using:\n",
    "   - Convolution layers for feature extraction  \n",
    "   - MaxPooling for dimensionality reduction  \n",
    "   - Dense layers for classification\n",
    "\n",
    "9. **Model Training**  \n",
    "   Trained the CNN model for a limited number of epochs to demonstrate rapid prototyping.\n",
    "\n",
    "10. **Model Evaluation**  \n",
    "    Evaluated the model on unseen test data and measured accuracy.\n",
    "\n",
    "11. **Prediction & Inference**  \n",
    "    Tested the trained model on single images and analyzed prediction probabilities.\n",
    "\n",
    "12. **Model Saving**  \n",
    "    Saved the trained model in `.h5` format for deployment and reuse.\n",
    "\n",
    "13. **Deployment Readiness**  \n",
    "    Exposed the trained model via a FastAPI backend to accept image input and return predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b3c8d-f1a6-4a3c-a818-9d612a186d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
